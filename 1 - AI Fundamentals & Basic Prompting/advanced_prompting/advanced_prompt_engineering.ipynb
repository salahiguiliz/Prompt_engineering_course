{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPEN THIS NOTEBOOK IN COLAB : [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/salahiguiliz/Prompt_engineering_course/blob/main/1%20-%20AI%20Fundamentals%20%26%20Basic%20Prompting/advanced_prompting/advanced_prompt_engineering.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Provider Agnostic Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note : \n",
    "Any LLm could be used here. But for simplification, we will use OpenAI's LLMs. However, we will only use the simple text completion API.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, if we want to use OpenAI python library, we need to install it. We can do this by running the following command in our terminal:\n",
    "\n",
    "```bash\n",
    "pip install openai\n",
    "```\n",
    "\n",
    "Or simply running the next cell in this notebook:\n",
    "- `%pip install openai` : install openai for the active python kernel\n",
    "- `-q` : quiet mode, suppresses output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.a Iinitialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()                                   # This loads the environment variables. Make sure to have a .env file with your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key found in environment variables.\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"OpenAI API key found in environment variables.\")\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "else : \n",
    "    print(\"OpenAI API key not found in environment variables. Please set it in a .env file.\")\n",
    "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API key: \")\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "#            Change these values, they are initially set fot o4-mini           #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# Add input tokens cost\n",
    "PER_MILLION_INPUT_TOKENS = 1.1      # 1.1 USD per million input tokens\n",
    "# Add output tokens cost\n",
    "PER_MILLION_OUTPUT_TOKENS = 4.4      # 4.4 USD per million output tokens\n",
    "\n",
    "def estimate_cost(response, input_token_cost=PER_MILLION_INPUT_TOKENS, output_token_cost=PER_MILLION_OUTPUT_TOKENS):\n",
    "    total_cost = 0\n",
    "\n",
    "    total_cost += (response.usage.input_tokens / 1_000_000) * input_token_cost\n",
    "    total_cost += (response.usage.output_tokens / 1_000_000) * output_token_cost\n",
    "\n",
    "    # print(f\"Request cost: ${total_cost:.4f}\")\n",
    "    print(f\"\\033[1mRequest cost: \\033[94m{total_cost:.4f} $\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def extract_info_from_response(resopnse):\n",
    "    # answer is between ```json ... ```\n",
    "    json_string = re.search(r'```json(.*?)```', resopnse.output[0].content[0].text, re.DOTALL).group(1)\n",
    "    json_string = json_string.strip()   # Remove any trailing whitespace\n",
    "    json_answer = json.loads(json_string)             # This will raise an error if the JSON is not valid. Maybe you should optimize the prompt or simply retry the request.\n",
    "    return json_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.b Function and Template definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calendar(user_id):\n",
    "    \"\"\"\n",
    "    This function gets the calendar of a user.\n",
    "    :param user_id: The ID of the user.\n",
    "    :return: The calendar of the user in a dictionary format.\n",
    "    \"\"\"\n",
    "    # For simplification, we are using a simple static calendar.\n",
    "    if user_id == 332:\n",
    "        calendar = {\n",
    "            \"Today\" : {\n",
    "                \"09:00-10:00\" : \"Meeting with Client\",\n",
    "                \"10:00-16:00\" : \"Work on Project\",\n",
    "                \"16:00-17:00\" : \"Call with Team\"\n",
    "            },\n",
    "            \"Tomorrow\" : {\n",
    "                \"10:00-11:00\" : \"Free for meetings\",\n",
    "                \"14:00-16:00\" : \"Work on Project\",\n",
    "                \"16:00-17:00\" : \"Call with Team\"\n",
    "            },\n",
    "        }\n",
    "    else: \n",
    "        calendar = {\n",
    "            \"Today\" : {\n",
    "                \"09:00-10:00\" : \"Meeting with Client\",\n",
    "                \"10:00-16:00\" : \"Work on Project\",\n",
    "                \"16:00-17:00\" : \"Call with Team\"\n",
    "            },\n",
    "            \"Tomorrow\" : {\n",
    "                \"09:00-16:00\" : \"Work on Project\",\n",
    "                \"16:00-17:00\" : \"Call with Team\"\n",
    "            },\n",
    "        }\n",
    "    return calendar\n",
    "\n",
    "def schedule_meeting(user_id, customer_id, start_time, end_time):\n",
    "    \"\"\"\n",
    "    This function schedules a meeting for a user.\n",
    "    :param user_id: The ID of the user.\n",
    "    :param customer_id: The ID of the customer.\n",
    "    :param start_time: The start time of the meeting.\n",
    "    :param end_time: The end time of the meeting.\n",
    "    :return: A confirmation message.\n",
    "    \"\"\"\n",
    "    # Some code here that  schedules a meeting.\n",
    "    ...\n",
    "    # For simplification, we are just returning a confirmation message.\n",
    "    print(f\"Meeting scheduled for user {user_id} with customer {customer_id} from {start_time} to {end_time}.\")\n",
    "    return True\n",
    "\n",
    "available_tools = {\n",
    "    \"get_calendar\": get_calendar,\n",
    "    \"schedule_meeting\": schedule_meeting\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools list the functions that can be called by the assistant.\n",
    "TOOLS = \"\"\"\n",
    "1. Function: `get_calendar`\n",
    "   Description: Given a user ID, it returns the user's availability.\n",
    "   Parameters:\n",
    "   - `user_id` (int): The ID of the user whose calendar should be retrieved.\n",
    "\n",
    "2. Function: `schedule_meeting`\n",
    "   Description: Schedules a meeting between a user and a customer.\n",
    "   Parameters:\n",
    "   - `user_id` (int): The ID of the user to schedule the meeting with.\n",
    "   - `customer_id` (int): The ID of the customer.\n",
    "   - `start_time` (string, ISO 8601 format): The start datetime of the meeting.\n",
    "   - `end_time` (string, ISO 8601 format): The end datetime of the meeting.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions are here to force the model to normalize the output to a specific format.\n",
    "INSTRUCTIONS = \"\"\"\n",
    "You must decide how to handle the input request. You have two options:\n",
    "\n",
    "### Option 1: Call a Function  \n",
    "If the task requires using one of the provided functions, respond **only** with the following JSON format:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"function_call\": {\n",
    "    \"name\": \"<function_name>\",\n",
    "    \"arguments\": {\n",
    "      \"<parameter_1>\": <value_1>,\n",
    "      \"<parameter_2>\": <value_2>,\n",
    "      ...\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### Option 2: Provide a Final Answer\n",
    "If the request can be fully handled without calling a function, return a final answer in this format:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"final_answer\": \"<your response to the user in natural language>\"\n",
    "}\n",
    "```\n",
    "Do not mix both formats. Choose one based on what is appropriate for the task. Do not add explanations or extra content outside the chosen JSON format.\n",
    "\"\"\"\n",
    "\n",
    "# The input contains the user's request\n",
    "INPUT = \"\"\"\n",
    "Schedule a meeting between \"Ali\" (customer_id = 223) and one of our managers sometime tomorrow:\n",
    "- Mohammad : user_id = 239\n",
    "- Soufiane : user_id = 332\n",
    "- Amine : user_id = 321\n",
    "\"\"\"\n",
    "\n",
    "# Will contain the execution\n",
    "INFO = \"\"\"\"\"\"\n",
    "\n",
    "# The generated prompt\n",
    "prompt_template = \"\"\"\n",
    "You are an intelligent assistant that can use external tools (functions) to help perform actions. You are provided with the following functions:\n",
    "\n",
    "FUNCTIONS:\n",
    "{TOOLS}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "{INSTRUCTIONS}\n",
    "\n",
    "INFO:\n",
    "{INFO}\n",
    "\n",
    "INPUT:\n",
    "{INPUT}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.c Start the function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mRequest cost: \u001b[94m0.0012 $\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "  model=\"gpt-4.1-2025-04-14\",\n",
    "  input=[{\n",
    "      \"role\": \"user\",\n",
    "      \"content\": prompt_template.format(TOOLS=TOOLS, INSTRUCTIONS=INSTRUCTIONS, INFO=INFO, INPUT=INPUT)\n",
    "  }]\n",
    ")\n",
    "estimate_cost(response, input_token_cost=2, output_token_cost=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'function_call': {'name': 'get_calendar', 'arguments': {'user_id': 239}}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_answer = extract_info_from_response(response)\n",
    "json_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, now the LLM asked us to provide more information. Lets provide it with the information it asked for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "get_calendar({'user_id': 239}) = {'Today': {'09:00-10:00': 'Meeting with Client', '10:00-16:00': 'Work on Project', '16:00-17:00': 'Call with Team'}, 'Tomorrow': {'09:00-16:00': 'Work on Project', '16:00-17:00': 'Call with Team'}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tools_answer = available_tools[json_answer[\"function_call\"][\"name\"]](\n",
    "    **json_answer[\"function_call\"][\"arguments\"]\n",
    ")\n",
    "INFO += f\"\"\"\n",
    "{json_answer[\"function_call\"][\"name\"]}({json_answer[\"function_call\"][\"arguments\"]}) = {tools_answer}\n",
    "\"\"\"\n",
    "print(INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mRequest cost: \u001b[94m0.0013 $\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "  model=\"gpt-4.1-2025-04-14\",\n",
    "  input=[{\n",
    "      \"role\": \"user\",\n",
    "      \"content\": prompt_template.format(TOOLS=TOOLS, INSTRUCTIONS=INSTRUCTIONS, INFO=INFO, INPUT=INPUT)\n",
    "  }]\n",
    ")\n",
    "estimate_cost(response, input_token_cost=2, output_token_cost=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'function_call': {'name': 'get_calendar', 'arguments': {'user_id': 332}}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_answer = extract_info_from_response(response)\n",
    "json_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.d Master the function calling with loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continously prompt the LLM until we reach a final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Assitant:\n",
    "    def __init__(self, \n",
    "                 tools=TOOLS, \n",
    "                 instructions=INSTRUCTIONS, \n",
    "                 input=INPUT, \n",
    "                 prompt_template=prompt_template, \n",
    "                 available_tools=available_tools, \n",
    "                 model=\"gpt-4.1-2025-04-14\",\n",
    "                 model_input_token_cost=2,\n",
    "                 model_output_token_cost=8, \n",
    "                 max_steps=15,\n",
    "                 client=client,\n",
    "                 verbose=False,\n",
    "        ):\n",
    "        self.tools = tools\n",
    "        self.instructions = instructions\n",
    "        self.input = input\n",
    "        self.prompt_template = prompt_template\n",
    "        self.available_tools = available_tools\n",
    "        self.model = model\n",
    "        self.model_input_token_cost = model_input_token_cost\n",
    "        self.model_output_token_cost = model_output_token_cost\n",
    "        self.max_steps = max_steps\n",
    "        self.client = client\n",
    "        self.verbose = verbose\n",
    "        self.info = \"\"\n",
    "        self.total_cost = 0\n",
    "    \n",
    "    def _estimate_cost(self, response):\n",
    "        \"\"\"\n",
    "        This function estimates the cost of a request.\n",
    "        :param response: The response from the model.\n",
    "        :return: The total cost of the request.\n",
    "        \"\"\"\n",
    "        self.total_cost += (response.usage.input_tokens / 1_000_000) * self.model_input_token_cost\n",
    "        self.total_cost += (response.usage.output_tokens / 1_000_000) * self.model_output_token_cost\n",
    "        return self.total_cost\n",
    "\n",
    "    def _step(self):\n",
    "        response = self.client.responses.create(\n",
    "          model=self.model,\n",
    "          input=[{\n",
    "              \"role\": \"user\",\n",
    "              \"content\": self.prompt_template.format(TOOLS=self.tools, INSTRUCTIONS=self.instructions, INFO=self.info, INPUT=self.input)\n",
    "          }]\n",
    "        )\n",
    "        self._estimate_cost(response)\n",
    "        json_answer = extract_info_from_response(response)\n",
    "        tools_answer = self.available_tools[json_answer[\"function_call\"][\"name\"]](\n",
    "            **json_answer[\"function_call\"][\"arguments\"]\n",
    "        )\n",
    "        self.info += f\"\"\"\n",
    "{json_answer[\"function_call\"][\"name\"]}({json_answer[\"function_call\"][\"arguments\"]}) = {tools_answer}\n",
    "\"\"\"\n",
    "        if self.verbose:\n",
    "            print(self.info)\n",
    "        return json_answer\n",
    "\n",
    "    def stop_condition(self, json_answer):\n",
    "        \"\"\"\n",
    "        The stop condition for the assistant.\n",
    "        When the the function \"schedule_meeting\" is called or when the final answer is provided.\n",
    "        \"\"\"\n",
    "        if \"final_answer\" in json_answer:       # The final answer, could be a message of impossibility or a confirmation.\n",
    "            return True\n",
    "        if json_answer[\"function_call\"][\"name\"] == \"schedule_meeting\":  # When the meeting is scheduled\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        This function runs the assistant until it reaches a stop condition or the maximum number of steps.\n",
    "        :return: The final answer from the assistant.\n",
    "        \"\"\"\n",
    "        for step in range(self.max_steps):\n",
    "            json_answer = self._step()\n",
    "            if self.stop_condition(json_answer):\n",
    "                break\n",
    "        return self.info, self.total_cost, step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put it in action for now. It should be indicating that the meeting have been scheduled with the user 332"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meeting scheduled for user 332 with customer 223 from 2024-06-07T10:00:00 to 2024-06-07T11:00:00.\n",
      "Total cost: 0.0044 $\n",
      "Total steps: 2\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "INFO :\n",
      "\n",
      "get_calendar({'user_id': 239}) = {'Today': {'09:00-10:00': 'Meeting with Client', '10:00-16:00': 'Work on Project', '16:00-17:00': 'Call with Team'}, 'Tomorrow': {'09:00-16:00': 'Work on Project', '16:00-17:00': 'Call with Team'}}\n",
      "\n",
      "get_calendar({'user_id': 332}) = {'Today': {'09:00-10:00': 'Meeting with Client', '10:00-16:00': 'Work on Project', '16:00-17:00': 'Call with Team'}, 'Tomorrow': {'10:00-11:00': 'Free for meetings', '14:00-16:00': 'Work on Project', '16:00-17:00': 'Call with Team'}}\n",
      "\n",
      "schedule_meeting({'user_id': 332, 'customer_id': 223, 'start_time': '2024-06-07T10:00:00', 'end_time': '2024-06-07T11:00:00'}) = True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assitant = Assitant(verbose=False)\n",
    "info, total_cost, steps = assitant.run()\n",
    "print(f\"Total cost: {total_cost:.4f} $\")\n",
    "print(f\"Total steps: {steps}\")\n",
    "print(\"--\"*100)\n",
    "print(\"INFO :\")\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Provider Specific Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. OpenAI Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, if we want to use OpenAI python library, we need to install it. We can do this by running the following command in our terminal:\n",
    "\n",
    "```bash\n",
    "pip install openai\n",
    "```\n",
    "\n",
    "Or simply running the next cell in this notebook:\n",
    "- `%pip install openai` : install openai for the active python kernel\n",
    "- `-q` : quiet mode, suppresses output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai python-dotenv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets initialize our libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()                                   # This loads the environment variables. Make sure to have a .env file with your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)         # In this case, we don't even need to pass the api_key, as the environment variable is already correctly set to \"OPENAI_API_KEY\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ResponseFunctionToolCall(arguments='{\"location\":\"Paris, France\"}', call_id='call_hXxifuyxZALVcGZLYAAtahHM', name='get_weather', type='function_call', id='fc_6833845fbaac8191b83fd3111985ce140f013326af7f7364', status='completed')]\n"
     ]
    }
   ],
   "source": [
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"Get current temperature for a given location.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"City and country e.g. Bogot√°, Colombia\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\n",
    "            \"location\"\n",
    "        ],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1-2025-04-14\",\n",
    "    input=[{\"role\": \"user\", \"content\": \"What is the weather like in Paris today?\"}],\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "print(response.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimation of the cost. Reasoning and Caching is not taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mRequest cost: \u001b[94m0.0001 $\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "total_cost = 0\n",
    "\n",
    "# Add input tokens cost\n",
    "PER_MILLION_INPUT_TOKENS = 1.1      # 1.1 USD per million input tokens\n",
    "total_cost += (response.usage.input_tokens / 1_000_000) * PER_MILLION_INPUT_TOKENS\n",
    "# Add output tokens cost\n",
    "PER_MILLION_OUTPUT_TOKENS = 4.4      # 4.4 USD per million output tokens\n",
    "total_cost += (response.usage.output_tokens / 1_000_000) * PER_MILLION_OUTPUT_TOKENS\n",
    "\n",
    "# print(f\"Request cost: ${total_cost:.4f}\")\n",
    "print(f\"\\033[1mRequest cost: \\033[94m{total_cost:.4f} $\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseUsage(input_tokens=59, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=17, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=76)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as you can see, we can simply repeat the same logic as before, but this time we will use the OpenAI function calling API. It is always helpful to create a specific class for the function calling or for each use case. FOr OpenAI, we recommend to follow the [OpenAI documentation for Function Calling](https://platform.openai.com/docs/guides/function-calling).\n",
    "\n",
    "The interest of using the Provider Specific Function Calling is that it is more efficient (cheaper) and it is more reliable. It is also more flexible as it allows you to define the function signature and the parameters that the LLM can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function Calling is independent of the LLM provider, it is even possible to use multiple providers mixed with some private or on-premise LLMs. However, it is more efficient and reliable to use the provider's specific function calling API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
