{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Provider Agnostic Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note : \n",
    "Any LLm could be used here. But for simplification, we will use OpenAI's LLMs. However, we will only use the simple text completion API.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, if we want to use OpenAI python library, we need to install it. We can do this by running the following command in our terminal:\n",
    "\n",
    "```bash\n",
    "pip install openai\n",
    "```\n",
    "\n",
    "Or simply running the next cell in this notebook:\n",
    "- `%pip install openai` : install openai for the active python kernel\n",
    "- `-q` : quiet mode, suppresses output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.a Iinitialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()                                   # This loads the environment variables. Make sure to have a .env file with your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)         # In this case, we don't even need to pass the api_key, as the environment variable is already correctly set to \"OPENAI_API_KEY\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "#            Change these values, they are initially set fot o4-mini           #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# Add input tokens cost\n",
    "PER_MILLION_INPUT_TOKENS = 1.1      # 1.1 USD per million input tokens\n",
    "# Add output tokens cost\n",
    "PER_MILLION_OUTPUT_TOKENS = 4.4      # 4.4 USD per million output tokens\n",
    "\n",
    "def estimate_cost(response, input_token_cost=PER_MILLION_INPUT_TOKENS, output_token_cost=PER_MILLION_OUTPUT_TOKENS):\n",
    "    total_cost = 0\n",
    "\n",
    "    total_cost += (response.usage.input_tokens / 1_000_000) * input_token_cost\n",
    "    total_cost += (response.usage.output_tokens / 1_000_000) * output_token_cost\n",
    "\n",
    "    # print(f\"Request cost: ${total_cost:.4f}\")\n",
    "    print(f\"\\033[1mRequest cost: \\033[94m{total_cost:.4f} $\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def extract_info_from_response(resopnse):\n",
    "    # answer is between ```json ... ```\n",
    "    json_string = re.search(r'```json(.*?)```', resopnse.output[0].content[0].text, re.DOTALL).group(1)\n",
    "    json_string = json_string.strip()   # Remove any trailing whitespace\n",
    "    json_answer = json.loads(json_string)             # This will raise an error if the JSON is not valid. Maybe you should optimize the prompt or simply retry the request.\n",
    "    return json_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.b Function and Template definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calendar(user_id):\n",
    "    \"\"\"\n",
    "    This function gets the calendar of a user.\n",
    "    :param user_id: The ID of the user.\n",
    "    :return: The calendar of the user in a dictionary format.\n",
    "    \"\"\"\n",
    "    # For simplification, we are using a simple static calendar.\n",
    "    if user_id == 332:\n",
    "        calendar = {\n",
    "            \"Today\" : {\n",
    "                \"09:00-10:00\" : \"Meeting with Client\",\n",
    "                \"10:00-16:00\" : \"Work on Project\",\n",
    "                \"16:00-17:00\" : \"Call with Team\"\n",
    "            },\n",
    "            \"Tomorrow\" : {\n",
    "                \"10:00-11:00\" : \"Free for meetings\",\n",
    "                \"14:00-16:00\" : \"Work on Project\",\n",
    "                \"16:00-17:00\" : \"Call with Team\"\n",
    "            },\n",
    "        }\n",
    "    else: \n",
    "        calendar = {\n",
    "            \"Today\" : {\n",
    "                \"09:00-10:00\" : \"Meeting with Client\",\n",
    "                \"10:00-16:00\" : \"Work on Project\",\n",
    "                \"16:00-17:00\" : \"Call with Team\"\n",
    "            },\n",
    "            \"Tomorrow\" : {\n",
    "                \"09:00-16:00\" : \"Work on Project\",\n",
    "                \"16:00-17:00\" : \"Call with Team\"\n",
    "            },\n",
    "        }\n",
    "    return calendar\n",
    "\n",
    "def schedule_meeting(user_id, customer_id, start_time, end_time):\n",
    "    \"\"\"\n",
    "    This function schedules a meeting for a user.\n",
    "    :param user_id: The ID of the user.\n",
    "    :param customer_id: The ID of the customer.\n",
    "    :param start_time: The start time of the meeting.\n",
    "    :param end_time: The end time of the meeting.\n",
    "    :return: A confirmation message.\n",
    "    \"\"\"\n",
    "    # Some code here that  schedules a meeting.\n",
    "    ...\n",
    "    # For simplification, we are just returning a confirmation message.\n",
    "    return f\"Meeting scheduled with {customer_id} from {start_time} to {end_time}.\"\n",
    "\n",
    "available_tools = {\n",
    "    \"get_calendar\": get_calendar,\n",
    "    \"schedule_meeting\": schedule_meeting\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools list the functions that can be called by the assistant.\n",
    "TOOLS = \"\"\"\n",
    "1. Function: `get_calendar`\n",
    "   Description: Given a user ID, it returns the user's availability.\n",
    "   Parameters:\n",
    "   - `user_id` (int): The ID of the user whose calendar should be retrieved.\n",
    "\n",
    "2. Function: `schedule_meeting`\n",
    "   Description: Schedules a meeting between a user and a customer.\n",
    "   Parameters:\n",
    "   - `user_id` (int): The ID of the user to schedule the meeting with.\n",
    "   - `customer_id` (int): The ID of the customer.\n",
    "   - `meeting_start` (string, ISO 8601 format): The start datetime of the meeting.\n",
    "   - `meeting_end` (string, ISO 8601 format): The end datetime of the meeting.\n",
    "\"\"\"\n",
    "\n",
    "# Instructions are here to force the model to normalize the output to a specific format.\n",
    "INSTRUCTIONS = \"\"\"\n",
    "You must decide how to handle the input request. You have two options:\n",
    "\n",
    "### Option 1: Call a Function  \n",
    "If the task requires using one of the provided functions, respond **only** with the following JSON format:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"function_call\": {\n",
    "    \"name\": \"<function_name>\",\n",
    "    \"arguments\": {\n",
    "      \"<parameter_1>\": <value_1>,\n",
    "      \"<parameter_2>\": <value_2>,\n",
    "      ...\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### Option 2: Provide a Final Answer\n",
    "If the request can be fully handled without calling a function, return a final answer in this format:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"final_answer\": \"<your response to the user in natural language>\"\n",
    "}\n",
    "```\n",
    "Do not mix both formats. Choose one based on what is appropriate for the task. Do not add explanations or extra content outside the chosen JSON format.\n",
    "\"\"\"\n",
    "\n",
    "# The input contains the user's request\n",
    "INPUT = \"\"\"\n",
    "Schedule a meeting between \"Ali\" (customer_id = 223) and one of our managers sometime tomorrow:\n",
    "- Mohammad : user_id = 239\n",
    "- Soufiane : user_id = 332\n",
    "- Amine : user_id = 321\n",
    "\"\"\"\n",
    "\n",
    "# Will contain the execution result of the function\n",
    "INFO = \"\"\"\"\"\"\n",
    "\n",
    "# The generated prompt\n",
    "prompt_template = \"\"\"\n",
    "You are an intelligent assistant that can use external tools (functions) to help perform actions. You are provided with the following functions:\n",
    "\n",
    "FUNCTIONS:\n",
    "{TOOLS}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "{INSTRUCTIONS}\n",
    "\n",
    "INFO:\n",
    "{INFO}\n",
    "\n",
    "INPUT:\n",
    "{INPUT}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.c Start the function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mRequest cost: \u001b[94m0.0012 $\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "  model=\"gpt-4.1-2025-04-14\",\n",
    "  input=[{\n",
    "      \"role\": \"user\",\n",
    "      \"content\": prompt_template.format(TOOLS=TOOLS, INSTRUCTIONS=INSTRUCTIONS, INFO=INFO, INPUT=INPUT)\n",
    "  }]\n",
    ")\n",
    "estimate_cost(response, input_token_cost=2, output_token_cost=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'function_call': {'name': 'get_calendar', 'arguments': {'user_id': 239}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_answer = extract_info_from_response(response)\n",
    "json_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, now we the LLM asked us to provide more information. Lets provide it with the information it asked for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "get_calendar({'user_id': 239}) = {'Today': {'09:00-10:00': 'Meeting with Client', '10:00-16:00': 'Work on Project', '16:00-17:00': 'Call with Team'}, 'Tomorrow': {'09:00-16:00': 'Work on Project', '16:00-17:00': 'Call with Team'}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tools_answer = available_tools[json_answer[\"function_call\"][\"name\"]](\n",
    "    **json_answer[\"function_call\"][\"arguments\"]\n",
    ")\n",
    "INFO += f\"\"\"\n",
    "{json_answer[\"function_call\"][\"name\"]}({json_answer[\"function_call\"][\"arguments\"]}) = {tools_answer}\n",
    "\"\"\"\n",
    "print(INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mRequest cost: \u001b[94m0.0013 $\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "  model=\"gpt-4.1-2025-04-14\",\n",
    "  input=[{\n",
    "      \"role\": \"user\",\n",
    "      \"content\": prompt_template.format(TOOLS=TOOLS, INSTRUCTIONS=INSTRUCTIONS, INFO=INFO, INPUT=INPUT)\n",
    "  }]\n",
    ")\n",
    "estimate_cost(response, input_token_cost=2, output_token_cost=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'function_call': {'name': 'get_calendar', 'arguments': {'user_id': 332}}}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_answer = extract_info_from_response(response)\n",
    "json_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.d Master the function calling with loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continously prompt the LLM until we reach a final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Assitant:\n",
    "    def __init__(self, \n",
    "                 tools=TOOLS, \n",
    "                 instructions=INSTRUCTIONS, \n",
    "                 input=INPUT, \n",
    "                 prompt_template=prompt_template, \n",
    "                 available_tools=available_tools, \n",
    "                 model=\"gpt-4.1-2025-04-14\",\n",
    "                 model_input_token_cost=2,\n",
    "                 model_output_token_cost=8, \n",
    "                 client=client\n",
    "        ):\n",
    "        self.tools = tools\n",
    "        self.instructions = instructions\n",
    "        self.input = input\n",
    "        self.prompt_template = prompt_template\n",
    "        self.available_tools = available_tools\n",
    "        self.model = model\n",
    "        self.model_input_token_cost = model_input_token_cost\n",
    "        self.model_output_token_cost = model_output_token_cost\n",
    "        self.client = client\n",
    "        self.info = \"\"\n",
    "        self.total_cost = 0\n",
    "    \n",
    "    def _estimate_cost(self, response):\n",
    "        \"\"\"\n",
    "        This function estimates the cost of a request.\n",
    "        :param response: The response from the model.\n",
    "        :return: The total cost of the request.\n",
    "        \"\"\"\n",
    "        self.total_cost += (response.usage.input_tokens / 1_000_000) * self.model_input_token_cost\n",
    "        self.total_cost += (response.usage.output_tokens / 1_000_000) * self.model_output_token_cost\n",
    "        return self.total_cost\n",
    "\n",
    "    def _step(self):\n",
    "        response = self.client.responses.create(\n",
    "          model=self.model,\n",
    "          input=[{\n",
    "              \"role\": \"user\",\n",
    "              \"content\": self.prompt_template.format(TOOLS=self.tools, INSTRUCTIONS=self.instructions, INFO=self.info, INPUT=self.input)\n",
    "          }]\n",
    "        )\n",
    "        self._estimate_cost(response)\n",
    "        json_answer = extract_info_from_response(response)\n",
    "        tools_answer = self.available_tools[json_answer[\"function_call\"][\"name\"]](\n",
    "            **json_answer[\"function_call\"][\"arguments\"]\n",
    "        )\n",
    "        self.info += f\"\"\"\n",
    "{json_answer[\"function_call\"][\"name\"]}({json_answer[\"function_call\"][\"arguments\"]}) = {tools_answer}\n",
    "\"\"\"\n",
    "        print(self.info)\n",
    "        return json_answer\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Provider Specific Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. OpenAI Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, if we want to use OpenAI python library, we need to install it. We can do this by running the following command in our terminal:\n",
    "\n",
    "```bash\n",
    "pip install openai\n",
    "```\n",
    "\n",
    "Or simply running the next cell in this notebook:\n",
    "- `%pip install openai` : install openai for the active python kernel\n",
    "- `-q` : quiet mode, suppresses output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai python-dotenv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets initialize our libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()                                   # This loads the environment variables. Make sure to have a .env file with your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)         # In this case, we don't even need to pass the api_key, as the environment variable is already correctly set to \"OPENAI_API_KEY\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ResponseReasoningItem(id='rs_68298f3e6df88191b420debcca8d619c0223165dfa925c2e', summary=[], type='reasoning', encrypted_content=None, status=None), ResponseFunctionToolCall(arguments='{\"location\":\"Paris, France\"}', call_id='call_9dBVgrjCYEvb5wktb8kQaok6', name='get_weather', type='function_call', id='fc_68298f3f8ce08191ba7292aac5ec9ad20223165dfa925c2e', status='completed')]\n"
     ]
    }
   ],
   "source": [
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"Get current temperature for a given location.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"City and country e.g. Bogotá, Colombia\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\n",
    "            \"location\"\n",
    "        ],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"o4-mini-2025-04-16\",\n",
    "    input=[{\"role\": \"user\", \"content\": \"What is the weather like in Paris today?\"}],\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "print(response.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimation of the cost. Reasoning and Caching is not taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mRequest cost: \u001b[94m0.0007 $\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "total_cost = 0\n",
    "\n",
    "# Add input tokens cost\n",
    "PER_MILLION_INPUT_TOKENS = 1.1      # 1.1 USD per million input tokens\n",
    "total_cost += (response.usage.input_tokens / 1_000_000) * PER_MILLION_INPUT_TOKENS\n",
    "# Add output tokens cost\n",
    "PER_MILLION_OUTPUT_TOKENS = 4.4      # 4.4 USD per million output tokens\n",
    "total_cost += (response.usage.output_tokens / 1_000_000) * PER_MILLION_OUTPUT_TOKENS\n",
    "\n",
    "# print(f\"Request cost: ${total_cost:.4f}\")\n",
    "print(f\"\\033[1mRequest cost: \\033[94m{total_cost:.4f} $\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseReasoningItem(id='rs_68298f3e6df88191b420debcca8d619c0223165dfa925c2e', summary=[], type='reasoning', encrypted_content=None, status=None),\n",
       " ResponseFunctionToolCall(arguments='{\"location\":\"Paris, France\"}', call_id='call_9dBVgrjCYEvb5wktb8kQaok6', name='get_weather', type='function_call', id='fc_68298f3f8ce08191ba7292aac5ec9ad20223165dfa925c2e', status='completed')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_68298f3dfda08191afe3eea9331c75b20223165dfa925c2e', created_at=1747554110.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='o4-mini-2025-04-16', object='response', output=[ResponseReasoningItem(id='rs_68298f3e6df88191b420debcca8d619c0223165dfa925c2e', summary=[], type='reasoning', encrypted_content=None, status=None), ResponseFunctionToolCall(arguments='{\"location\":\"Paris, France\"}', call_id='call_9dBVgrjCYEvb5wktb8kQaok6', name='get_weather', type='function_call', id='fc_68298f3f8ce08191ba7292aac5ec9ad20223165dfa925c2e', status='completed')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='get_weather', parameters={'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'City and country e.g. Bogotá, Colombia'}}, 'required': ['location'], 'additionalProperties': False}, strict=True, type='function', description='Get current temperature for a given location.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort='medium', generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=61, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=150, output_tokens_details=OutputTokensDetails(reasoning_tokens=128), total_tokens=211), user=None, store=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
